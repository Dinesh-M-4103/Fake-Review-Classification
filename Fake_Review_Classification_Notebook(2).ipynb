{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"16IncCZsl13oC22Te163nQYuHpRMwkww5","timestamp":1735638976883}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2385Z3jDJV9","executionInfo":{"status":"ok","timestamp":1735188189899,"user_tz":-330,"elapsed":23961,"user":{"displayName":"Dinesh M","userId":"14011036212119232130"}},"outputId":"b4c926fa-9f47-4cbb-f54e-7b503aadf01b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["# Import necessary libraries\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import normalize\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.cluster import KMeans, DBSCAN\n","from sklearn.metrics import silhouette_score, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import RobertaTokenizer, TFRobertaForSequenceClassification, RobertaConfig\n","from sklearn.neighbors import NearestNeighbors\n","import matplotlib.pyplot as plt\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["nltk.download('punkt_tab')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LS1jgV_wRl0f","executionInfo":{"status":"ok","timestamp":1735188191495,"user_tz":-330,"elapsed":1606,"user":{"displayName":"Dinesh M","userId":"14011036212119232130"}},"outputId":"122c4adf-0dec-4a4b-f1da-793e0b732577"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["# Load dataset\n","df = pd.read_csv(\"/content/fake reviews dataset (5).csv\")"],"metadata":{"id":"QpsQLZJiRrKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocessing\n","lemmatizer = WordNetLemmatizer()\n","stemmer = PorterStemmer()\n","stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text(text):\n","    tokens = nltk.word_tokenize(text)\n","    tokens = [word for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n","    tokens = [word.lower() for word in tokens if word not in stop_words]  # Remove stopwords\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n","    tokens = [stemmer.stem(word) for word in tokens]  # Stemming\n","    return ' '.join(tokens)\n","\n","df['cleaned_text'] = df['text_'].apply(preprocess_text)"],"metadata":{"id":"twjNAhhPRpi2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TF-IDF Vectorization\n","tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n","X = tfidf.fit_transform(df['cleaned_text']).toarray()\n","X_normalized = normalize(X)"],"metadata":{"id":"JhSnjAiMRqQq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Classification: Logistic Regression and SVM\n","X_class = tfidf.transform(df['cleaned_text']).toarray()\n","y_class = df['label']\n","X_train, X_test, y_train, y_test = train_test_split(X_class, y_class, test_size=0.3, random_state=42)"],"metadata":{"id":"6oHE72FPRyK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LSTM Model\n","tokenizer = Tokenizer(num_words=5000)\n","tokenizer.fit_on_texts(df['cleaned_text'])\n","X_seq = tokenizer.texts_to_sequences(df['cleaned_text'])\n","X_padded = pad_sequences(X_seq, maxlen=200)\n","\n","X_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_padded, y_class, test_size=0.3, random_state=42)\n","le = LabelEncoder()\n","y_train_dl = le.fit_transform(y_train_dl)\n","y_test_dl = le.transform(y_test_dl)\n","\n","model = Sequential()\n","model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))\n","model.add(Bidirectional(LSTM(64, return_sequences=False)))\n","model.add(Dropout(0.5))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(X_train_dl, y_train_dl, epochs=5, batch_size=64, validation_split=0.2)\n","lstm_results = model.evaluate(X_test_dl, y_test_dl)\n","print(\"LSTM Results:\\n\", lstm_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VYVBdeDR-TD","executionInfo":{"status":"ok","timestamp":1735188383765,"user_tz":-330,"elapsed":77853,"user":{"displayName":"Dinesh M","userId":"14011036212119232130"}},"outputId":"ccdc9298-063a-485b-ae8a-bc4180e83c5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 28ms/step - accuracy: 0.7474 - loss: 0.5122 - val_accuracy: 0.8977 - val_loss: 0.2407\n","Epoch 2/5\n","\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9200 - loss: 0.2040 - val_accuracy: 0.9096 - val_loss: 0.2199\n","Epoch 3/5\n","\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 21ms/step - accuracy: 0.9427 - loss: 0.1517 - val_accuracy: 0.9094 - val_loss: 0.2215\n","Epoch 4/5\n","\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 20ms/step - accuracy: 0.9552 - loss: 0.1145 - val_accuracy: 0.9164 - val_loss: 0.2210\n","Epoch 5/5\n","\u001b[1m354/354\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - accuracy: 0.9664 - loss: 0.0938 - val_accuracy: 0.9140 - val_loss: 0.2571\n","\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step - accuracy: 0.9119 - loss: 0.2746\n","LSTM Results:\n"," [0.2740212082862854, 0.9079967141151428]\n"]}]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import RobertaTokenizer, TFRobertaForSequenceClassification, RobertaConfig\n","from sklearn.preprocessing import LabelEncoder # Import LabelEncoder\n","\n","tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n","train_encodings = tokenizer_roberta(list(df['cleaned_text']), truncation=True, padding=True, max_length=200, return_tensors=\"tf\")\n","X_train_roberta, X_test_roberta, y_train_roberta, y_test_roberta = train_test_split(\n","    np.array(train_encodings['input_ids']),\n","    y_class,\n","    test_size=0.3,\n","    random_state=42\n",")\n","le = LabelEncoder()\n","y_train_roberta = le.fit_transform(y_train_roberta)\n","y_test_roberta = le.transform(y_test_roberta)\n","config = RobertaConfig.from_pretrained('roberta-base', num_labels=2)\n","model_roberta = TFRobertaForSequenceClassification.from_pretrained('roberta-base', config=config)\n","model_roberta.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n","model_roberta.fit(X_train_roberta, y_train_roberta, epochs=3, batch_size=8) # Now using numerical labels\n","roberta_results = model_roberta.evaluate(X_test_roberta, y_test_roberta) # Now using numerical labels\n","print(\"RoBERTa Results:\\n\", roberta_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uw6zVha7Spb0","executionInfo":{"status":"ok","timestamp":1735192466256,"user_tz":-330,"elapsed":3891676,"user":{"displayName":"Dinesh M","userId":"14011036212119232130"}},"outputId":"db982096-06ce-4317-c9d0-57e40245140e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n","- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","3538/3538 [==============================] - 1288s 346ms/step - loss: 0.7050 - accuracy: 0.4983\n","Epoch 2/3\n","3538/3538 [==============================] - 1215s 344ms/step - loss: 0.6980 - accuracy: 0.4962\n","Epoch 3/3\n","3538/3538 [==============================] - 1214s 343ms/step - loss: 0.6964 - accuracy: 0.5032\n","380/380 [==============================] - 154s 397ms/step - loss: 0.6971 - accuracy: 0.5001\n","RoBERTa Results:\n"," [0.6970633864402771, 0.5000824332237244]\n"]}]}]}